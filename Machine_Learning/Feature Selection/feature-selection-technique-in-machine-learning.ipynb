{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "<h2><div style=\"font-family: Trebuchet MS; background-color: red; color: #FFFFFF; padding: 12px; line-height: 1.5;\"> Feature Selection</div></h2> \n",
    "Feature Selection is one of the most import technique for a great predictive model. It help us to know the most important features of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8710c1c227abfd06369844ebc57af6fca32b4632"
   },
   "source": [
    "<h3><div style=\"font-family: Trebuchet MS; background-color:#176BA0;; color: #FFFFFF; padding: 10px; line-height: 1.5;\">1. | I will cover the below points : üåü üìö</div></h3>\n",
    "\n",
    "1. What is Feature Selection?\n",
    "2. Why it is one the most important techinque to learn for a Data Scientitst?\n",
    "3. What are the different type of Feature Selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c755f87fc7ca150d89268ee9dba94b2720d69657"
   },
   "source": [
    "<h10><div style=\"font-family: Trebuchet MS; background-color:orange; color: #FFFFFF; padding: 1px; line-height: 1.;\">1.1 | 1.Feature Selection: üåç:</div></h10>\n",
    "\n",
    "The process of selecting subset of relevant features for use in model construction which will help to increase the model prediction and decrease the error rate. \n",
    "In other word you can say its a  process of identifying and removing as much of  irrelevant and redundent information as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8779a7d4886a84ff86204ac95a0f6eba11876b58"
   },
   "source": [
    "<h10><div style=\"font-family: Trebuchet MS; background-color:orange; color: #FFFFFF; padding: 1px; line-height: 1.;\">1.1 | 2. Importance of Feature Selection:üåç:</div></h10>\n",
    "\n",
    "* Improve the accuracy of model.\n",
    "* Reduce overfitting.\n",
    "* Shoter traning time.\n",
    "* Reduce complexity of model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1411d4d61851c8e1f404cc3275aec9fbc46b3ba8",
    "collapsed": true
   },
   "source": [
    "\n",
    "<h10><div style=\"font-family: Trebuchet MS; background-color:orange; color: #FFFFFF; padding: 1px; line-height: 1.;\"> Type of Feature Selection </div></h10>\n",
    "\n",
    "*         ***Wrapper Method***\n",
    "*         ***Filter Method***\n",
    "*         ***Embedded Method***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "79ba39eefedbbc4a5ee444d9b88f953d261663fd"
   },
   "source": [
    "\n",
    "<h3><div style=\"font-family: Trebuchet MS; background-color:#176BA0;; color: #FFFFFF; padding: 10px; line-height: 1.5;\">  Wrapper Method üåü üìö</div></h3>\n",
    "\n",
    "\n",
    "In this method a subset of features are selected and train a model using them. Based on the inference that we draw from the previous model, we decide to add or remove features from subset.\n",
    "[For indepth details](https://en.wikipedia.org/wiki/Feature_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "503925da9749631ebb3a0e07be5587a8583060a1"
   },
   "source": [
    "**Image from wiki**\n",
    "<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/0/04/Feature_selection_Wrapper_Method.png\" alt=\"Feature selection Wrapper Method.png\" height=\"179\" width=\"640\"><br></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e3a24d030c50a3cc50559c03f586bf2288c221f4",
    "collapsed": true
   },
   "source": [
    "\n",
    "<h10><div style=\"font-family: Trebuchet MS; background-color:orange; color: #FFFFFF; padding: 1px; line-height: 1.;\">  Type of Wrapper Method</div></h10>\n",
    "\n",
    "\n",
    "* Forward Selection\n",
    "* Backward Elimination\n",
    "* Exhaustive Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2fa60b67f8e4805bcd497bf464d925b64b1900b5"
   },
   "source": [
    "<h10><div style=\"font-family: Trebuchet MS; background-color:orange; color: #FFFFFF; padding: 1px; line-height: 1.;\">  Forward Selection</div></h10>\n",
    "\n",
    "It is a iterative method in which we keep adding feature which  best improves our model till an addition of a new feature does not improve the model performance.<br/><br/>\n",
    "<h10><div style=\"font-family: Trebuchet MS; background-color:orange; color: #FFFFFF; padding: 1px; line-height: 1.;\">  Backward Elimination</div></h10>\n",
    "In this we start with all features and removes the least significant feature at each iteration which improves the model performance. We repeat this until no improvemnt is observed on removal of feature.<br><br>\n",
    "\n",
    "<h10><div style=\"font-family: Trebuchet MS; background-color:orange; color: #FFFFFF; padding: 1px; line-height: 1.;\">  Exhaustive Feature Selection</div></h10>\n",
    "\n",
    "In this the best subset of feature is selected, over all possible feature subsets. For example, if a dataset contains 4 features, the algorithm will evaluate all the feature combinations as follows:\n",
    "* All possible combinations of 1  feature\n",
    "* All possible combinations of 2 features\n",
    "* All possible combinations of 3 features\n",
    "* All possible combinations of 4 features\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eafcc42656451003439a83ee28839b1408d274cb"
   },
   "source": [
    "\n",
    "<h10><div style=\"font-family: Trebuchet MS; background-color:orange; color: #FFFFFF; padding: 1px; line-height: 1.;\">  Pros</div></h10>\n",
    "\n",
    "\n",
    "* Aim to find the best possible feature combintaion.\n",
    "* Better result then filter method.\n",
    "* Can we used for small dataset having less features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9fad7d1d69b81bb7e9157860890f028ff8483c82"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<h10><div style=\"font-family: Trebuchet MS; background-color:orange; color: #FFFFFF; padding: 1px; line-height: 1.;\">  Cons</div></h10>\n",
    "\n",
    "* Computationally expensive\n",
    "* Often impracticable for large dataset having more features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1c6e339fd12bf741b1d3e00edd2c2ee3c136ea7d"
   },
   "source": [
    "<h3><div style=\"font-family: Trebuchet MS; background-color:#176BA0;; color: #FFFFFF; padding: 10px; line-height: 1.5;\">  Filter Method  üìö</div></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7d104eeabb0bd4ad80a59a9c66d88f5487a88937"
   },
   "source": [
    "Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0913e455d8c335346b3fe9099d00105e8a65c1b8",
    "collapsed": true
   },
   "source": [
    "**Image from wiki**\n",
    "<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/2/2c/Filter_Methode.png\" alt=\"Filter Methode.png\" height=\"63\" width=\"640\"></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "211f9335e79a2ae0ca5a30ec96945f39e7481d07"
   },
   "source": [
    "\n",
    "\n",
    "<h10><div style=\"font-family: Trebuchet MS; background-color:red; color: #FFFFFF; padding: 1px; line-height: 1.;\">  Basic Methods</div></h10>\n",
    "\n",
    "We should consider the below filter methods as a data pre processing steps.\n",
    "* Constant features - Constant features are those that show the same value for all the observations of the dataset. Remove constant features from dataset.\n",
    "* Quasi-constant features  - The column which contain 99% of same data is called Quasi constant column. Remove Quasi constant features from dataset.\n",
    "* Duplicated features - Remove duplicated features from dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fce63551ebc4a8d6b671c9a0c4f10ac89d4fdff4"
   },
   "source": [
    "\n",
    "<h10><div style=\"font-family: Trebuchet MS; background-color:red; color: #FFFFFF; padding: 1px; line-height: 1.;\"> Correlation</div></h10>\n",
    "\n",
    "* Correlation is measure of the linear relationship of 2 or more variables.\n",
    "* Through correlation we can predict one variable from other.\n",
    "    * Good variables are highly correlated with the target but uncorrelated among themselves.\n",
    "* If two variables are highly correlated with each other, then we should remove one of them.   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d007d540b45ed89c084a571db241329c03bd30b3"
   },
   "source": [
    "\n",
    "<h10><div style=\"font-family: Trebuchet MS; background-color:red; color: #FFFFFF; padding: 1px; line-height: 1.;\">  Fisher Score</div></h10>\n",
    "\n",
    "* Measures the dependence of 2 variables\n",
    "* Suited for categorical variables.\n",
    "* Target should be binary.\n",
    "* Variable values should be non negative, typically Boolean or counts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "59c38c2392896e016e2ab499b2ac829475d6cc19"
   },
   "source": [
    "\n",
    "\n",
    "<h10><div style=\"font-family: Trebuchet MS; background-color:red; color: #FFFFFF; padding: 1px; line-height: 1.;\">  ANOVA (Analysis Of Variance)</div></h10>\n",
    "\n",
    "* Measures the dependency of two variables.\n",
    "* Suited for continuous variables.\n",
    "* Requires a binary target.\n",
    "* Assumes linear relationship between variable and target.\n",
    "* Assumes variables are normally distributed.\n",
    "* Sensitive to sample size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "987b7c270f4b43373b3d70fa216de2f866e64c03"
   },
   "source": [
    "<h10><div style=\"font-family: Trebuchet MS; background-color:red; color: #FFFFFF; padding: 1px; line-height: 1.;\">  ROC-AUC / RMSE</div></h10>\n",
    "\n",
    "* Measures the dependency of two variables.\n",
    "* Suited for all type of variables.\n",
    "* Makes no assumption on the distribution of the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d817284e30d6bdb48e9508e3ecdf4fb015d087f2"
   },
   "source": [
    "\n",
    "<h10><div style=\"font-family: Trebuchet MS; background-color:red; color: #FFFFFF; padding: 1px; line-height: 1.;\">  Steps to  select features</div></h10>\n",
    "\n",
    "* Rank features according to a certain criteria (like correlation).\n",
    "    * Each feature is ranked independently of the feature space.\n",
    "* Select highest ranking features.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4ddc34ce1df6366a605729d04ca48094ade7009"
   },
   "source": [
    "\n",
    "<h10><div style=\"font-family: Trebuchet MS; background-color:red; color: #FFFFFF; padding: 1px; line-height: 1.;\">  Basic Pros</div></h10>\n",
    "\n",
    "* Fast computation.\n",
    "* Simple yet powerful to quickly remove irrelevant and redundant feature.\n",
    "* Better choice for large dataset over wrapper methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "def3fa408a109f6d51bb1663a3bbcf5e66a231fd"
   },
   "source": [
    "<h10><div style=\"font-family: Trebuchet MS; background-color:red; color: #FFFFFF; padding: 1px; line-height: 1.;\">  Basic Cons</div></h10>\n",
    "\n",
    "* It may select redundant variables because they do not consider the relationships between features.\n",
    "* The prediction accuracy is lesser than wrapper methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "87d76ab632364c86325f8b7749409bb77bf20432"
   },
   "source": [
    "\n",
    "<h3><div style=\"font-family: Trebuchet MS; background-color:#176BA0;; color: #FFFFFF; padding: 10px; line-height: 1.5;\"> Embedded Method  üåü üìö</div></h3>\n",
    "\n",
    "\n",
    "Embedded method combine the features of Filter and Wrapper methods. A learning algorithm takes advantage of its own variable selection process and performs feature selection and classification simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b61d42f474e71dec29b9ff15752f106a21459c7e"
   },
   "source": [
    "**Image from wiki**\n",
    "<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/b/bf/Feature_selection_Embedded_Method.png\" alt=\"Feature selection Embedded Method.png\" height=\"190\" width=\"640\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "17807564445dd5b351e939e8309c60501785b693"
   },
   "source": [
    "<h3><div style=\"font-family: Trebuchet MS; background-color:#176BA0;; color: #FFFFFF; padding: 10px; line-height: 1.5;\"> REGULARISATION üåü üìö</div></h3>\n",
    "\n",
    "\n",
    "\n",
    "Regularization consists in adding a penalty on the different parameters of the model to reduce the freedom of the model. Hence, the model will be less likely to fit the noise of the training data and will improve the generalization abilities of the model. For linear models there are in general 3 types of regularisation:\n",
    "* The L1 regularization (also called Lasso)\n",
    "* The L2 regularization (also called Ridge)\n",
    "* The L1/L2 regularization (also called Elastic net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "10c4a1d2541d1bfcbc3a6c378624be3f4ca9879a"
   },
   "source": [
    "**Image from Scikit learn**\n",
    "<p><img src=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_sgd_penalties_001.png\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9a0bf904c39487ae6b3cc26a9053100526224d72",
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6675c83145ea22d7780e15a19b074c90884eebd4",
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c1e5227338ce91fb45514340eca51cbd4130c6a4",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
